A decision tree is a flow-chart-like structure, where each internal (non-leaf) node denotes a test on an attribute, 
each branch represents the outcome of a test, and each leaf (or terminal) node holds a class label. 
The topmost node in a tree is the root node.

Why chose decision trees?

Different kinds of models have different advantages. 
The decision tree model is very good at handling tabular data with numerical features, or categorical features with fewer 
than hundreds of categories. 
Unlike linear models, decision trees are able to capture non-linear interaction between the features and the target.
One important note is that tree based models are not designed to work with very sparse features. 
When dealing with sparse input data (e.g. categorical features with large dimension), we can either pre-process the 
sparse features to generate numerical statistics, or switch to a linear model, which is better suited for such scenarios.

