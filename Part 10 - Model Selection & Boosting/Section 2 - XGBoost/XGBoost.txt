XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. 
It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting 
(also known as GBDT, GBM) that solve many data science problems in a fast and accurate way. 
The same code runs on major distributed environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples.

Speed and performance : Originally written in C++, it is comparatively faster than other ensemble classifiers.

-Core algorithm is parallelizable : Because the core XGBoost algorithm is parallelizable it can harness the power of 
                                    multi-core computers. It is also parallelizable onto GPU’s and across networks of 
				    computers making it feasible to train on very large datasets as well.
-Consistently outperforms other algorithm methods : It has shown better performance on a variety of machine learning 
						    benchmark datasets.
-Wide variety of tuning parameters : XGBoost internally has parameters for cross-validation, regularization, user-defined 
				     objective functions, missing values, tree parameters, scikit-learn compatible API etc.

=>XGBoost (Extreme Gradient Boosting) belongs to a family of boosting algorithms and uses the gradient boosting (GBM) 
  framework at its core. It is an optimized distributed gradient boosting library.